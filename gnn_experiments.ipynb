{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2c8385e9-dc0f-4cbc-ae4c-da8a5c918176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Embedding\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import GATConv\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "07370475-8432-4f26-9b57-7e43800bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental setting\n",
    "relative_time = False\n",
    "semi_transductive = True\n",
    "heterogenous_msg_passing = True\n",
    "dataset_path = \"data/act-mooc/graph.pt\"\n",
    "#dataset_path = \"data/junyi/graph.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8433a9f7-4066-4c68-8ae8-88bdbdc4a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "emb_dim = 50\n",
    "hidden_dim = 50\n",
    "time_dim = 50\n",
    "identity_dim = 25\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "759e88e9-306e-4559-b399-bfb8b74769d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 823498], node_id=[7144], edge_attr=[823498, 4], time=[823498], edge_y=[823498], node_type=[7144], edge_type=[823498], x=[7144, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(dataset_path)\n",
    "data = data.to_homogeneous()\n",
    "data.x = torch.zeros(data.num_nodes, 1)\n",
    "data.edge_attr = data.edge_attr.float()\n",
    "data.time = data.time.float()\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ecbb950-1a37-422a-92d6-431365d49702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_in_batches(edge_index, nodes, batch_size):\n",
    "    mask = torch.zeros(edge_index.size(1), dtype=torch.bool, device=device)\n",
    "    for i in range(0, len(nodes), batch_size):\n",
    "        batch_nodes = nodes[i:i + batch_size]\n",
    "        batch_mask = ((edge_index[0].unsqueeze(1) == batch_nodes).any(dim=1)) | \\\n",
    "                     ((edge_index[1].unsqueeze(1) == batch_nodes).any(dim=1))\n",
    "        mask |= batch_mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6228f2b8-62e9-4afb-9630-eba67fde3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new Data object from the original data and a mask\n",
    "def create_subset(data, mask):\n",
    "    # Filter data using the mask\n",
    "    subset = Data()\n",
    "    for key, item in data:\n",
    "        if key in ['node_id', 'node_type', 'x']:\n",
    "            subset[key] = item\n",
    "        elif key in ['edge_index']:\n",
    "            subset[key] = item[:,mask]\n",
    "        else:\n",
    "            subset[key] = item[mask]\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36bfcac6-3e8b-4a14-bc24-248b8d584a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if relative_time:\n",
    "    unique_node_ids = data.node_id[data.node_type==0].unique()\n",
    "    min_times_per_node = {node_id: data.time[data.edge_index[0,:] == node_id].min() for node_id in unique_node_ids}\n",
    "    \n",
    "    # Subtract the minimum time from each node's times\n",
    "    for node_id, min_time in min_times_per_node.items():\n",
    "        data.time[data.edge_index[0,:] == node_id] -= min_time\n",
    "        data.time[data.edge_index[1,:] == node_id] -= min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c4f1f40b-5aae-4f0a-b112-33dfc9157859",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = data.time.cpu().numpy()\n",
    "quantile_70 = np.quantile(time_array, 0.7)\n",
    "quantile_85 = np.quantile(time_array, 0.85)\n",
    "\n",
    "# Create masks for splitting the data\n",
    "train_mask = time_array < quantile_70\n",
    "val_mask = time_array < quantile_85\n",
    "test_mask = np.ones_like(time_array, dtype=bool)\n",
    "\n",
    "# Create subsets\n",
    "train_data = create_subset(data, train_mask)\n",
    "val_data = create_subset(data, val_mask)\n",
    "test_data = create_subset(data, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb49e10-f293-4464-8a57-c7a68555bda5",
   "metadata": {},
   "source": [
    "This class iterates over batches of users and returns all edges they are connected with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eee577e3-00f2-467e-a659-017f9106eba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df6dc7a1-75cc-4ab0-b254-9aad86600d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TemporalLoader(train_data, 0, batch_size=batch_size)\n",
    "val_loader = TemporalLoader(val_data, quantile_70, batch_size=batch_size)\n",
    "test_loader = TemporalLoader(test_data, quantile_85, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b0bfb00-4a60-4e29-aab7-16f56d457ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7144, 50])\n",
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "class TimeEncoder(torch.nn.Module):\n",
    "    def __init__(self, time_dim):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.time_lin = Linear(1, time_dim)\n",
    "\n",
    "    def forward(self, t: Tensor):\n",
    "        return self.time_lin(t.view(-1, 1)).cos()\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_channels, out_channels, edge_dim, time_enc, resource_ids):\n",
    "        super().__init__()\n",
    "        self.conv1_user_to_resource = GATConv(emb_dim, hidden_channels, edge_dim=edge_dim)\n",
    "        self.conv2_user_to_resource = GATConv(hidden_channels, out_channels, edge_dim=edge_dim)\n",
    "        \n",
    "        if heterogenous_msg_passing:\n",
    "            self.conv1_resource_to_user = GATConv(emb_dim, hidden_channels, edge_dim=edge_dim)\n",
    "            self.conv2_resource_to_user = GATConv(hidden_channels, out_channels, edge_dim=edge_dim)\n",
    "            \n",
    "        self.time_enc = time_enc\n",
    "        if semi_transductive:\n",
    "            self.mapping = {nid: i+1 for i, nid in enumerate(resource_ids)}\n",
    "            self.embedding = Embedding(len(resource_ids)+1, emb_dim)\n",
    "        else:\n",
    "            self.mapping = {nid: 1 for i, nid in enumerate(resource_ids)}\n",
    "            self.embedding = Embedding(2, emb_dim)\n",
    "\n",
    "    def forward(self, node_id: Tensor, node_type: Tensor, edge_index: Tensor, edge_attr: Tensor, t: Tensor, edge_type: Tensor) -> Tensor:\n",
    "        # x: Node feature matrix of shape [num_nodes, in_channels]\n",
    "        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n",
    "        # edge_attr: Edge features\n",
    "        # t: Timestamp of edges\n",
    "        time_enc = self.time_enc(t)\n",
    "        edge_attr = torch.cat([time_enc, edge_attr], dim=-1)\n",
    "        \n",
    "\n",
    "        node_ids = torch.tensor([self.mapping.get(nid, 0) for nid in node_id]).to(device)\n",
    "        x = self.embedding(node_ids)\n",
    "\n",
    "        mask = edge_type == 0\n",
    "        \n",
    "        if heterogenous_msg_passing:\n",
    "            x_resources = self.conv1_user_to_resource(x, edge_index[:, mask], edge_attr[mask]).relu()\n",
    "            x_users = self.conv1_resource_to_user(x, edge_index[:, ~mask], edge_attr[~mask]).relu()\n",
    "            users = node_type == 0\n",
    "            h = torch.zeros_like(x_users)\n",
    "            h[users] = x_users[users]\n",
    "            h[~users] = x_resources[~users]\n",
    "        else:\n",
    "            h = self.conv1_user_to_resource(x, edge_index, edge_attr).relu()\n",
    "\n",
    "        if heterogenous_msg_passing:\n",
    "            h_resources = self.conv2_user_to_resource(h, edge_index[:, mask], edge_attr[mask]).relu()\n",
    "            h_users = self.conv2_resource_to_user(h, edge_index[:, ~mask], edge_attr[~mask]).relu()\n",
    "            o = torch.zeros_like(h_users)\n",
    "            o[users,:] = h_users[users,:]\n",
    "            o[~users,:] = h_resources[~users,:]\n",
    "        else: \n",
    "            o = self.conv2_user_to_resource(h, edge_index, edge_attr).relu()\n",
    "        \n",
    "        return o\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, time_enc):\n",
    "        super().__init__()\n",
    "        self.time_enc = time_enc\n",
    "        dim = 2 * in_channels + time_enc.time_dim # 2*\n",
    "        self.lin = Linear(dim, dim)\n",
    "        self.lin_final = Linear(dim, 1)\n",
    "\n",
    "    def forward(self, z_src, z_dst, t):\n",
    "        time_enc = self.time_enc(t)\n",
    "        input = torch.cat([z_src, z_dst, time_enc], dim=-1)\n",
    "        h = self.lin(input)\n",
    "        h = h.relu()\n",
    "        \n",
    "        return self.lin_final(h)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "time_enc = TimeEncoder(time_dim).to(device)\n",
    "gnn = GATModel(identity_dim, hidden_dim, emb_dim, time_dim+data.edge_attr.size(1), time_enc, data.node_id[data.node_type == 1]).to(device)\n",
    "link_pred = LinkPredictor(emb_dim, time_enc).to(device)\n",
    "\n",
    "# Test forward run of the models\n",
    "batch = next(iter(train_loader)).to(device)\n",
    "train_loader.reset() # reset\n",
    "embs = gnn(batch.node_id, batch.node_type, batch.edge_index, batch.edge_attr, batch.time, batch.edge_type)\n",
    "print(embs.shape)\n",
    "link_preds = link_pred(embs[batch.edge_index[0]], embs[batch.edge_index[1]], batch.time)\n",
    "print(link_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "749fc235-4aee-4c67-b429-52838832d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21650"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in gnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bb196a3e-07b4-4b21-9d04-031d96fc6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(gnn, link_pred, time_enc, loader):\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    time_enc.eval()\n",
    "\n",
    "    true_labels = torch.Tensor().to(device)\n",
    "    predictions = torch.Tensor().to(device)\n",
    "    batches = 0\n",
    "    for batch in tqdm(loader, desc='Batches'):\n",
    "        batches += 1\n",
    "        mask = (batch.edge_y < 0)\n",
    "        node_embs = gnn(batch.node_id, batch.node_type, batch.edge_index[:,mask], batch.edge_attr[mask], batch.time[mask], batch.edge_type[mask])\n",
    "        mask = (batch.edge_y >= 0) & (batch.edge_type == 1)\n",
    "        link_preds = link_pred(node_embs[batch.edge_index[0,mask]], node_embs[batch.edge_index[1,mask]], batch.time[mask]).sigmoid()\n",
    "\n",
    "        predictions = torch.cat((predictions, link_preds[:,0].detach()), dim=0)\n",
    "        true_labels = torch.cat((true_labels, batch.edge_y[mask]), dim=0)\n",
    "    if batches == 0:\n",
    "        return test(gnn, link_pred, time_enc, loader)\n",
    "    loss = criterion(predictions, true_labels)\n",
    "    auc = roc_auc_score(true_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    ap = average_precision_score(true_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    return loss, auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f0571ccc-5e7f-4978-8123-34595f79bd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6339dd72-fbef-47f2-baa0-1c66f7cd8817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [02:19<00:00, 16.12it/s]\n",
      "Batches:   0%|          | 0/482 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:24<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.47399836778640747, Val Loss: 0.9258419871330261, Val AUC: 0.7419130624533772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [02:20<00:00, 16.08it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:24<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.462100625038147, Val Loss: 0.918373703956604, Val AUC: 0.744299491923845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [02:18<00:00, 16.27it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:24<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.4628545045852661, Val Loss: 0.9034318923950195, Val AUC: 0.7417200345296964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [02:00<00:00, 18.63it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.463765949010849, Val Loss: 0.9231296181678772, Val AUC: 0.7374693507289175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [02:21<00:00, 15.86it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:25<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.4584500193595886, Val Loss: 0.9090256690979004, Val AUC: 0.741870813562711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2251/2251 [01:53<00:00, 19.87it/s]\n",
      "Batches:   0%|          | 0/482 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:26<00:00, 18.05it/s]\n",
      "Training Batches: 100%|██████████| 2251/2251 [02:16<00:00, 16.50it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.47761037945747375, Val Loss: 0.9328076839447021, Val AUC: 0.7423107339418967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 26.16it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.46269509196281433, Val Loss: 0.9034401178359985, Val AUC: 0.738573397393995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:27<00:00, 25.79it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:15<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.4641542434692383, Val Loss: 0.9101966023445129, Val AUC: 0.7432027991612384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 26.02it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.4641079604625702, Val Loss: 0.914806067943573, Val AUC: 0.740816706143825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 25.96it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.46169334650039673, Val Loss: 0.9138443470001221, Val AUC: 0.7395095219418661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2251/2251 [01:05<00:00, 34.56it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:15<00:00, 31.89it/s]\n",
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 26.14it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.4961237609386444, Val Loss: 0.9304590821266174, Val AUC: 0.7292831435097245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 26.13it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.4638499915599823, Val Loss: 0.8991677165031433, Val AUC: 0.7424007271231149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:27<00:00, 25.85it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.4524284303188324, Val Loss: 0.9032407402992249, Val AUC: 0.7574341064704088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 26.01it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.4366084933280945, Val Loss: 0.8871848583221436, Val AUC: 0.759292641098798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2251/2251 [01:26<00:00, 25.93it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:14<00:00, 32.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.44076114892959595, Val Loss: 0.8817405700683594, Val AUC: 0.7631146997300685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2251/2251 [01:05<00:00, 34.53it/s]\n",
      "Batches: 100%|██████████| 482/482 [00:15<00:00, 31.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    time_enc = TimeEncoder(time_dim).to(device)\n",
    "    gnn = GATModel(identity_dim, hidden_dim, emb_dim, time_dim+data.edge_attr.size(1), time_enc, data.node_id[data.node_type == 1]).to(device)\n",
    "    link_pred = LinkPredictor(emb_dim, time_enc).to(device)\n",
    "    optimizer = torch.optim.Adam(set(time_enc.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        gnn.train()\n",
    "        link_pred.train()\n",
    "        time_enc.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc='Training Batches'):\n",
    "            batch.to(device)\n",
    "            # forward pass with edges that are older than this batch\n",
    "            mask = batch.edge_y < 0\n",
    "            node_embs = gnn(batch.node_id, batch.node_type, batch.edge_index[:,mask], batch.edge_attr[mask], batch.time[mask], batch.edge_type[mask])\n",
    "            \n",
    "            # first predict for all dropout edges\n",
    "            mask = (batch.edge_y == 1) & (batch.edge_type == 1)\n",
    "            positive_edges = batch.edge_index[:, mask]\n",
    "            if positive_edges.size(1) == 0: continue\n",
    "            pos_out = link_pred(node_embs[positive_edges[0]], node_embs[positive_edges[1]], batch.time[mask])\n",
    "            loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "    \n",
    "            # sample the same number of 0 edges from edge_index\n",
    "            negative_indices = torch.nonzero((batch.edge_y == 0) & (batch.edge_type == 1)).squeeze()\n",
    "            negative_indices = negative_indices[torch.randperm(negative_indices.size(0))][:positive_edges.size(1)]\n",
    "            negative_edges = batch.edge_index[:, negative_indices]\n",
    "            neg_out = link_pred(node_embs[negative_edges[0]], node_embs[negative_edges[1]], batch.time[negative_indices])\n",
    "            loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "    \n",
    "            # backward pass and gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "    \n",
    "        val_loss, val_auc, val_ap = test(gnn, link_pred, time_enc, val_loader)\n",
    "        train_loss = total_loss / (2 * len(train_loader))\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}, Val AUC: {val_auc}')\n",
    "        \n",
    "    train_loss, train_auc, train_ap = test(gnn, link_pred, time_enc, train_loader)\n",
    "    test_loss, test_auc, test_ap = test(gnn, link_pred, time_enc, test_loader)\n",
    "    \n",
    "    append_to_csv(\"results.csv\", train_loss.item(), val_loss.item(), test_loss.item(), train_auc, val_auc, test_auc, train_ap, val_ap, test_ap, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43225af-0e32-4a0a-b1e7-f45fbbf6833e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
